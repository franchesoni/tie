{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first compute pred patches using challenge_conv1d.py, then postprocess with this notebook\n",
    "data = np.load('tmp/pred_patches.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe429b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['casing'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729675b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "well = 10\n",
    "section = 0\n",
    "patch = 0\n",
    "patches = []\n",
    "while True:\n",
    "    name = f'well_{well}_section_0_patch_{patch}'\n",
    "    if name in data['tie']:\n",
    "        patches.append(data['tie'][name])\n",
    "        patch += 1\n",
    "    else:\n",
    "        break\n",
    "patches = np.concatenate(patches, axis=0)\n",
    "plt.imshow(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize score map\n",
    "import torch\n",
    "def compute_score_map(img, ball_size=3):\n",
    "    \"\"\"Compute score map, assume second dim is time\"\"\"\n",
    "    assert ball_size % 2 == 1, \"Ball size must be odd\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # build the terrain\n",
    "    timg = torch.from_numpy(img)\n",
    "    terrain = torch.cat(\n",
    "        (torch.zeros(1, img.shape[1]), timg, torch.zeros(1, img.shape[1])), dim=0\n",
    "    ).to(device)\n",
    "    timg = timg.to(device)\n",
    "    terrain_down = terrain.clone()\n",
    "    terrain_up = terrain.clone()\n",
    "    maxpool = torch.nn.MaxPool1d(ball_size, stride=1, padding=ball_size // 2)\n",
    "    for row_ind in range(1, terrain.shape[0] - 1):  # forward\n",
    "        # now the terrain has the max value that can be achieved from top to the current row\n",
    "        terrain_down[row_ind] = (\n",
    "            maxpool(terrain_down[row_ind - 1 : row_ind])[0] + terrain_down[row_ind]\n",
    "        )\n",
    "    for row_ind in range(terrain.shape[0] - 2, 0, -1):  # backward\n",
    "        # now the terrain has the max value that can be achieved from bottom to the current row\n",
    "        terrain_up[row_ind] = (\n",
    "            maxpool(terrain_up[row_ind + 1 : row_ind + 2])[0] + terrain_up[row_ind]\n",
    "        )\n",
    "    terrain = (terrain_up + terrain_down)[\n",
    "        1:-1\n",
    "    ] - timg  # remove the padding and the original image$\n",
    "    terrain = terrain / terrain.shape[0]  # mean node on path\n",
    "    return terrain.cpu().numpy()\n",
    "\n",
    "score_map = compute_score_map(patches)\n",
    "plt.figure()\n",
    "plt.imshow((score_map == np.max(score_map, axis=1, keepdims=True)) + score_map / score_map.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference!\n",
    "kernel_size = 21\n",
    "wells = sorted(list(set([name.split('_')[1] for name in data['casing'].keys()])))\n",
    "predictions = dict()\n",
    "for well in wells:\n",
    "    sections = sorted(list(set([name.split('_')[3] for name in data['casing'].keys() if f\"well_{well}\" in name])))\n",
    "    for section in sections:\n",
    "        print(f'well_{well}_section_{section}')\n",
    "        patch = 0\n",
    "        casings, ties = [], []\n",
    "        while True:\n",
    "            name = f\"well_{well}_section_{section}_patch_{patch}\"\n",
    "            if name in data['casing']:\n",
    "                casing = data['casing'][name]\n",
    "                tie = data['tie'][name]\n",
    "                casings.append(casing)\n",
    "                ties.append(tie)\n",
    "            else:\n",
    "                break\n",
    "            patch += 1\n",
    "        casings = np.concatenate(casings, axis=0)\n",
    "        ties = np.concatenate(ties, axis=0)\n",
    "\n",
    "        casing_score = compute_score_map(casings)\n",
    "        casing_line = casing_score == np.max(casing_score, axis=1, keepdims=True)\n",
    "        \n",
    "        tie_score = compute_score_map(ties)\n",
    "        tie_line = tie_score == np.max(tie_score, axis=1, keepdims=True)\n",
    "\n",
    "        out_label = np.clip(casing_line * 2 + tie_line * 1, min=0, max=2)\n",
    "        out_label = torch.nn.MaxPool2d(kernel_size=(1, kernel_size), stride=1, padding=(0, kernel_size//2))(torch.from_numpy(out_label).float()[None, None])[0, 0]\n",
    "\n",
    "        for patch_number in range(patch):\n",
    "            patch_name = f\"well_{well}_section_{section}_patch_{patch_number}\"\n",
    "            predictions[patch_name] = out_label[patch_number*160:(patch_number+1)*160, :].numpy().astype(int).flatten()\n",
    "    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(out_label)\n",
    "print('saving...')\n",
    "pd.DataFrame(predictions, dtype=\"int\").T.to_csv('y_test_csv_file_line_21.csv')\n",
    "print('saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b54901",
   "metadata": {},
   "source": [
    "## enforce casing < tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e573bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_probabilities_batched(prob_A, prob_B, w=0):\n",
    "    \"\"\"\n",
    "    Compute P(B=b | A + w < B) and P(A=a | A + w < B) for batched probability distributions\n",
    "\n",
    "    Args:\n",
    "        prob_A: Array of shape (batch_size, N) containing P(A=a) distributions\n",
    "        prob_B: Array of shape (batch_size, N) containing P(B=b) distributions\n",
    "        w: Integer offset (default: 0, which gives A < B condition)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (P(B=b | A + w < B), P(A=a | A + w < B)) both of shape (batch_size, N)\n",
    "    \"\"\"\n",
    "    # Sanity checks\n",
    "    if prob_A.shape != prob_B.shape:\n",
    "        raise ValueError(\"Input arrays must have the same shape\")\n",
    "\n",
    "    batch_size, N = prob_A.shape\n",
    "\n",
    "    # Check if distributions sum to 1\n",
    "    sum_A = np.sum(prob_A, axis=1)\n",
    "    sum_B = np.sum(prob_B, axis=1)\n",
    "    if not np.all((0.99 <= sum_A) & (sum_A <= 1.01)) or not np.all(\n",
    "        (0.99 <= sum_B) & (sum_B <= 1.01)\n",
    "    ):\n",
    "        raise ValueError(\"Input arrays must contain valid probability distributions\")\n",
    "\n",
    "    # Compute P(A + w < b) for all b\n",
    "    # This means A < b - w, so we need P(A < b-w)\n",
    "    prob_A_less_than_offset = np.zeros_like(prob_A)\n",
    "\n",
    "    # For each position b, we need the cumulative sum of A up to index b-w-1\n",
    "    for b in range(N):\n",
    "        threshold = b - w\n",
    "        if threshold > 0:\n",
    "            prob_A_less_than_offset[:, b] = np.sum(prob_A[:, :threshold], axis=1)\n",
    "\n",
    "    # Compute P(B > a + w) for all a\n",
    "    prob_B_greater_than_offset = np.zeros_like(prob_B)\n",
    "\n",
    "    # For each position a, we need the cumulative sum of B from index a+w+1 to the end\n",
    "    for a in range(N):\n",
    "        threshold = a + w + 1\n",
    "        if threshold < N:\n",
    "            prob_B_greater_than_offset[:, a] = np.sum(prob_B[:, threshold:], axis=1)\n",
    "\n",
    "    # Compute P(B=b, A + w < B) for all b\n",
    "    joint_prob_B = prob_B * prob_A_less_than_offset\n",
    "\n",
    "    # Compute P(A=a, A + w < B) for all a\n",
    "    joint_prob_A = prob_A * prob_B_greater_than_offset\n",
    "\n",
    "    # Compute P(A + w < B) = sum of all joint probabilities\n",
    "    prob_A_plus_w_less_B = np.sum(joint_prob_B, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute conditional probabilities, handling division by zero\n",
    "    conditional_prob_B = np.zeros_like(prob_B)\n",
    "    conditional_prob_A = np.zeros_like(prob_A)\n",
    "\n",
    "    # Use np.divide with where parameter to handle division by zero\n",
    "    np.divide(\n",
    "        joint_prob_B,\n",
    "        prob_A_plus_w_less_B,\n",
    "        out=conditional_prob_B,\n",
    "        where=prob_A_plus_w_less_B > 0,\n",
    "    )\n",
    "    np.divide(\n",
    "        joint_prob_A,\n",
    "        prob_A_plus_w_less_B,\n",
    "        out=conditional_prob_A,\n",
    "        where=prob_A_plus_w_less_B > 0,\n",
    "    )\n",
    "\n",
    "    return conditional_prob_B, conditional_prob_A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecdc840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference!\n",
    "kernel_size = 21\n",
    "wells = sorted(list(set([name.split('_')[1] for name in data['casing'].keys()])))\n",
    "predictions = dict()\n",
    "for well in wells:\n",
    "    sections = sorted(list(set([name.split('_')[3] for name in data['casing'].keys() if f\"well_{well}\" in name])))\n",
    "    for section in sections:\n",
    "        print(f'well_{well}_section_{section}')\n",
    "        patch = 0\n",
    "        casings, ties = [], []\n",
    "        while True:\n",
    "            name = f\"well_{well}_section_{section}_patch_{patch}\"\n",
    "            if name in data['casing']:\n",
    "                casing = data['casing'][name]\n",
    "                tie = data['tie'][name]\n",
    "                casings.append(casing)\n",
    "                ties.append(tie)\n",
    "            else:\n",
    "                break\n",
    "            patch += 1\n",
    "        casings = np.concatenate(casings, axis=0)\n",
    "        ties = np.concatenate(ties, axis=0)\n",
    "        ties, _ = compute_conditional_probabilities_batched(casings, ties, w=40)\n",
    "\n",
    "        casing_score = compute_score_map(casings)\n",
    "        casing_line = casing_score == np.max(casing_score, axis=1, keepdims=True)\n",
    "        \n",
    "        tie_score = compute_score_map(ties)\n",
    "        tie_line = tie_score == np.max(tie_score, axis=1, keepdims=True)\n",
    "\n",
    "        out_label = np.clip(casing_line * 2 + tie_line * 1, min=0, max=2)\n",
    "        out_label = torch.nn.MaxPool2d(kernel_size=(1, kernel_size), stride=1, padding=(0, kernel_size//2))(torch.from_numpy(out_label).float()[None, None])[0, 0]\n",
    "\n",
    "        for patch_number in range(patch):\n",
    "            patch_name = f\"well_{well}_section_{section}_patch_{patch_number}\"\n",
    "            predictions[patch_name] = out_label[patch_number*160:(patch_number+1)*160, :].numpy().astype(int).flatten()\n",
    "    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(out_label)\n",
    "print('saving...')\n",
    "pd.DataFrame(predictions, dtype=\"int\").T.to_csv('y_test_csv_file_conditioned_line_21.csv')\n",
    "print('saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873ad3f",
   "metadata": {},
   "source": [
    "## now enforce cosine regularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cosines import inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a05e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference!\n",
    "kernel_size = 21\n",
    "wells = sorted(list(set([name.split('_')[1] for name in data['casing'].keys()])))\n",
    "predictions = dict()\n",
    "for well in wells:\n",
    "    sections = sorted([int(e) for e in set([name.split('_')[3] for name in data['casing'].keys() if f\"well_{well}\" in name])])\n",
    "    images = {'tie': [], 'casing': []}\n",
    "    depth_score_maps_casing = []\n",
    "    depth_score_maps_tie = []\n",
    "    for section in sections:\n",
    "        print(f'well_{well}_section_{section}', end='\\r')\n",
    "        patch = 0\n",
    "        casings, ties = [], []\n",
    "        while True:\n",
    "            name = f\"well_{well}_section_{section}_patch_{patch}\"\n",
    "            if name in data['casing']:\n",
    "                casing = data['casing'][name]\n",
    "                tie = data['tie'][name]\n",
    "                casings.append(casing)\n",
    "                ties.append(tie)\n",
    "            else:\n",
    "                break\n",
    "            patch += 1\n",
    "        casings = np.concatenate(casings, axis=0)\n",
    "        ties = np.concatenate(ties, axis=0)\n",
    "        images['casing'].append(casings)\n",
    "        images['tie'].append(ties)\n",
    "\n",
    "    images = {'casing': np.array(images['casing']), 'tie': np.array(images['tie'])}\n",
    "    A, R, T = images['casing'].shape\n",
    "\n",
    "    final_score_maps_casing = inference(images['casing'])[0]\n",
    "    final_score_maps_tie = inference(images['tie'])[0]\n",
    "\n",
    "    casing_lines = final_score_maps_casing == np.max(final_score_maps_casing, axis=2, keepdims=True)\n",
    "    tie_lines = final_score_maps_tie == np.max(final_score_maps_tie, axis=2, keepdims=True)\n",
    "    print(out_label.shape, casing_lines.shape, tie_lines.shape)\n",
    "    out_label = np.clip(casing_lines * 2 + tie_lines * 1, min=0, max=2)\n",
    "    out_label = torch.nn.MaxPool2d(kernel_size=(1, kernel_size), stride=1, padding=(0, kernel_size//2))(torch.from_numpy(out_label).float()[:, None])[:, 0]\n",
    "\n",
    "    for sind, section in enumerate(sections):\n",
    "        for patch_number in range(patch):\n",
    "            patch_name = f\"well_{well}_section_{section}_patch_{patch_number}\"\n",
    "            predictions[patch_name] = out_label[section][patch_number*160:(patch_number+1)*160, :].numpy().astype(int).flatten()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(out_label[0])\n",
    "print('saving...')\n",
    "pd.DataFrame(predictions, dtype=\"int\").T.to_csv('y_test_csv_file_line_21_cosine.csv')\n",
    "print('saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
